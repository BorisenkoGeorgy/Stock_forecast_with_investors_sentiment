{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c433ce4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "from configparser import ConfigParser\n",
    "import datetime\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pytorch_lightning as L\n",
    "from pytorch_lightning.loggers.neptune import NeptuneLogger\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from torchmetrics import F1Score, Accuracy\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from models.code import CRNN, CNN, CBiRNN, CTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e68c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ConfigParser()\n",
    "config.read(['../configs/neptune_secret.ini'], encoding='utf-8')\n",
    "\n",
    "cfg = {\n",
    "    'debug': True,\n",
    "    'seed': 123,\n",
    "    'device': 'gpu' if torch.cuda.is_available() else 'cpu',\n",
    "    'n_devices': torch.cuda.device_count() if torch.cuda.is_available() else 1,\n",
    "    'companies': ['CHMF', 'GAZP', 'GMKN', 'LKOH', 'MOEX', 'NLMK', 'NVTK', 'PLZL', 'ROSN', 'SBER', 'SNGS', 'T', 'TATN'],\n",
    "\n",
    "    'batch_size': 32,\n",
    "    'epochs': 100,\n",
    "    'accamulate_grad_batches': 1,\n",
    "    'lr': 3e-4,\n",
    "    'mixed': True,\n",
    "    'patience': 10,\n",
    "    'save_top_k': 5,\n",
    "\n",
    "    'weight_decay': 1e-2,\n",
    "    'max_grad_norm': 1e6,\n",
    "\n",
    "    'sentiment': True,\n",
    "    'model_name': 'CNN',\n",
    "    'emb_type': 'vk',\n",
    "    'out_dir': 'D:/Jora/Аспирантура/Stock_forecast_with_investors_sentiment/models/weights/'\n",
    "}\n",
    "\n",
    "models = {\n",
    "    'CNN': CNN,\n",
    "    'CRNN': CRNN,\n",
    "    'CBiRNN': CBiRNN,\n",
    "    'CTransformer': CTransformer\n",
    "}\n",
    "\n",
    "base_model = models[cfg['model_name']]()\n",
    "\n",
    "if cfg['debug']:\n",
    "    kwargs = {'limit_train_batches': 2,\n",
    "              'limit_val_batches': 2, \n",
    "              'num_sanity_val_steps': 0}\n",
    "    cfg['batch_size'] = 2\n",
    "    cfg['epochs'] = 2\n",
    "    cfg['mixed'] = False if cfg['device'] == 'cpu' else True\n",
    "else:\n",
    "    kwargs = {}\n",
    "\n",
    "os.makedirs(cfg['out_dir'] + cfg['model_name'], exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c03a11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PulseSent(L.LightningModule):\n",
    "\n",
    "    def __init__(self, model, lr, epochs, out_dir=cfg['out_dir']):\n",
    "        super(PulseSent, self).__init__()\n",
    "\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.loss = nn.MSELoss()\n",
    "        dev = 'cuda' if cfg['device'] == 'gpu' else 'cpu'\n",
    "        self.metrics = {'f1': F1Score(task='binary').to(dev), 'acc': Accuracy(task='binary').to(dev)}\n",
    "        self.out_dir = out_dir\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def all_gather_reshape(self, data_lst):\n",
    "        \"\"\"Collects tensors from all GPUs and reshapes them to (NUM_GPUS*N, D, H, W)\"\"\"\n",
    "        data_lst = list(self.all_gather(data_lst))\n",
    "        for i in range(len(data_lst)):\n",
    "            data_lst[i] = data_lst[i].reshape(-1, *data_lst[i].shape[2:])\n",
    "        return data_lst\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        x, y = batch\n",
    "        out = self(x)\n",
    "        loss = self.loss(out, y)\n",
    "        self.log(f\"train_loss\", 100*loss.sqrt(), on_step=False, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        self._shared_step(batch, 'val')\n",
    "\n",
    "    def test_step(self, batch):\n",
    "        self._shared_step(batch, 'test')\n",
    "\n",
    "    def _shared_step(self, batch, name):\n",
    "        x, y = batch\n",
    "        out = self(x)\n",
    "        val_loss = self.loss(out, y)\n",
    "        if cfg['n_devices'] > 1:\n",
    "            out, y = self.all_gather_reshape((out, y))\n",
    "        for metric in self.metrics:\n",
    "            self.metrics[metric].update((out > 0).type(torch.int), (y > 0).type(torch.int))\n",
    "            self.log(f\"{name}_{metric}\", self.metrics[metric].compute(), on_step=False, on_epoch=True, logger=True, prog_bar=True, sync_dist=True)\n",
    "        self.log(f\"{name}_loss\", 100*val_loss.sqrt(), on_step=False, on_epoch=True, logger=True, prog_bar=True, sync_dist=True)\n",
    "\n",
    "\n",
    "    def predict_step(self, x):\n",
    "        return self(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr, weight_decay=cfg['weight_decay'])\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1eb0975",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentDataset(Dataset):\n",
    "\n",
    "    def __init__(self, df_train: pl.DataFrame, n_lags: int, df_val: pl.DataFrame=None, target_word='diff', add_embs=cfg['sentiment']):\n",
    "        super().__init__()\n",
    "        df_with_lags = self.create_lags(df_train, n_lags, df_val)\n",
    "\n",
    "        target_cols = [col for col in df_with_lags.columns if target_word in col and 'lag' not in col]\n",
    "        ts_cols = [col for col in df_with_lags.columns if target_word in col and 'lag' in col]\n",
    "        \n",
    "        ts = df_with_lags.select(ts_cols).to_torch(dtype=pl.Float32).view((len(df_with_lags), n_lags, -1))\n",
    "\n",
    "        if add_embs:\n",
    "            emb_cols = [col for col in df_with_lags.columns if 'column' in col and 'lag' in col]\n",
    "            embs = df_with_lags.select(emb_cols).to_torch(dtype=pl.Float32).view((len(df_with_lags), n_lags, -1))\n",
    "            self.X = torch.cat([embs, ts], dim=-1)\n",
    "        else:\n",
    "            self.X = ts\n",
    "\n",
    "        self.y = df_with_lags.select(target_cols).to_torch(dtype=pl.Float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, ind):\n",
    "        return self.X[ind], self.y[ind]\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_lags(df_train: pl.DataFrame, n_lags: int, df_val: pl.DataFrame=None):\n",
    "        price_cols = [col for col in df_train.columns if 'diff' in col]\n",
    "        emb_cols = [col for col in df_train.columns if 'column' in col]\n",
    "\n",
    "        if df_val is not None:\n",
    "            val_dates = df_val['date']\n",
    "            df_train = pl.concat([df_train, df_val])\n",
    "        \n",
    "        lag_expressions = []\n",
    "\n",
    "        for lag in range(1, n_lags + 1):\n",
    "            for pc in price_cols:\n",
    "                lag_expressions.append(\n",
    "                    pl.col(pc).shift(lag).alias(f'{pc}_lag_{lag}')\n",
    "                )\n",
    "        \n",
    "        for lag in range(1, n_lags+1):\n",
    "            for ec in emb_cols:\n",
    "                lag_expressions.append(\n",
    "                    pl.col(ec).shift(lag).alias(f'{ec}_lag_{lag}')\n",
    "                )\n",
    "\n",
    "        df_with_lags = df_train.with_columns(lag_expressions)\n",
    "\n",
    "        if df_val is not None:\n",
    "            df_with_lags = df_with_lags.filter(pl.col('date').is_in(val_dates.implode())).sort('date')\n",
    "        \n",
    "        return df_with_lags.drop_nulls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8d6bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pl.read_parquet('../data/final/GAZP.parquet').drop(['weighted_1d', 'open', 'close', 'high', 'low'])\n",
    "\n",
    "# df_train = df.filter(pl.col('date') <= datetime.date(2024, 4, 30))\n",
    "# df_val = df.filter(pl.col('date') > datetime.date(2024, 4, 30),\n",
    "#                    pl.col('date') <= datetime.date(2024, 10, 31))\n",
    "# df_test = df.filter(pl.col('date') > datetime.date(2024, 10, 31))\n",
    "\n",
    "# ds_train = SentDataset(df_train, 10)\n",
    "# ds_val = SentDataset(df_train, 10, df_val)\n",
    "# ds_test = SentDataset(df_val, 10, df_test)\n",
    "\n",
    "# dl_train = DataLoader(ds_train, batch_size=32, shuffle=True, drop_last=True, pin_memory=True)\n",
    "# dl_val = DataLoader(ds_val, batch_size=32, shuffle=False, pin_memory=True)\n",
    "# dl_test = DataLoader(ds_test, batch_size=32, shuffle=False, pin_memory=True)\n",
    "\n",
    "# for batch in dl_train:\n",
    "#     break\n",
    "\n",
    "# batch[0].shape, batch[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c26b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate(model_version: str, company: str):\n",
    "\n",
    "    df = pl.read_parquet(f'../data/final/{company}.parquet').drop(['weighted_1d', 'open', 'close', 'high', 'low'])\n",
    "\n",
    "    df_train = df.filter(pl.col('date') <= datetime.date(2024, 4, 30))\n",
    "    df_val = df.filter(pl.col('date') > datetime.date(2024, 4, 30),\n",
    "                    pl.col('date') <= datetime.date(2024, 10, 31))\n",
    "    df_test = df.filter(pl.col('date') > datetime.date(2024, 10, 31))\n",
    "\n",
    "    if not cfg['debug']:\n",
    "        neptune_logger = NeptuneLogger(\n",
    "            project=config['NEPTUNE']['project'],\n",
    "            api_key=config['NEPTUNE']['key'],\n",
    "            log_model_checkpoints=False,\n",
    "            name=cfg['model_name'] + f'_{company}' + (f'{cfg[\"sentiment\"]}' if cfg[\"sentiment\"] else '')\n",
    "            )\n",
    "    else:\n",
    "        neptune_logger = None\n",
    "\n",
    "    if not cfg['debug']:\n",
    "        neptune_logger.log_hyperparams(cfg) if neptune_logger else None\n",
    "\n",
    "    ds_train = SentDataset(df_train, 10)\n",
    "    ds_val = SentDataset(df_train, 10, df_val)\n",
    "    ds_test = SentDataset(df_val, 10, df_test)\n",
    "\n",
    "    dl_train = DataLoader(ds_train, batch_size=32, shuffle=True, drop_last=True, pin_memory=True)\n",
    "    dl_val = DataLoader(ds_val, batch_size=32, shuffle=False, pin_memory=True)\n",
    "    dl_test = DataLoader(ds_test, batch_size=32, shuffle=False, pin_memory=True)\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=cfg['out_dir'],\n",
    "        monitor=f\"val_loss_{i}\",\n",
    "        save_top_k=cfg['save_top_k'],\n",
    "        save_last=False,\n",
    "        save_weights_only=True,\n",
    "        filename=f\"{company}_fold_{i}\" + \"_epoch_{epoch}_\" + \"{val_loss:.4f}\",\n",
    "        save_on_train_epoch_end=False,\n",
    "        verbose=False,\n",
    "        auto_insert_metric_name=False,\n",
    "        mode=\"min\",\n",
    "    )\n",
    "\n",
    "    early_stop_callback = EarlyStopping(\n",
    "        monitor=f\"val_loss_{i}\", min_delta=0.00, patience=cfg['patience'], verbose=False, mode=\"min\")\n",
    "\n",
    "    model = PulseSent(\n",
    "        model=base_model,\n",
    "        lr=cfg['lr'],\n",
    "        epochs=cfg['epochs'],\n",
    "        steps_per_epoch=len(train_loader),\n",
    "        out_dir=cfg['out_dir']\n",
    "    )\n",
    "\n",
    "    trainer = L.Trainer(\n",
    "        logger=False if cfg['debug'] else neptune_logger,\n",
    "        max_epochs=cfg['epochs'],\n",
    "        deterministic=False,\n",
    "        accumulate_grad_batches=cfg['accamulate_grad_batches'],\n",
    "        accelerator=cfg['device'],\n",
    "        devices=cfg['n_devices'],\n",
    "        callbacks=[checkpoint_callback, early_stop_callback],\n",
    "        precision='16-mixed' if cfg['mixed'] else 32,\n",
    "        gradient_clip_val=cfg['max_grad_norm'],\n",
    "        log_every_n_steps=1,\n",
    "        enable_model_summary=False,\n",
    "        enable_progress_bar=False,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    trainer.fit(\n",
    "        model=model,\n",
    "        train_dataloaders=dl_train,\n",
    "        val_dataloaders=dl_val\n",
    "    )\n",
    "\n",
    "    best_model = PulseSent.load_from_checkpoint(checkpoint_callback.best_model_path)\n",
    "    best_model.eval()\n",
    "    trainer.test(best_model, df_test)\n",
    "\n",
    "    del trainer, model\n",
    "    gc.collect()\n",
    "\n",
    "if not cfg['debug']:\n",
    "    neptune_logger.experiment.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
